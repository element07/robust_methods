---
---
---

## Wykorzystanie metod statystyki odpornej do modelowania cen mieszkań

Wykorzystywany zbiór danych: <https://www.kaggle.com/datasets/mokar2001/house-price-tehran-iran>

### Wykorzystane Bibilioteki

```{r}
# r version: 4.2.3
library(dplyr)
library(ggplot2)
#library(caret)
#library(solitude)
library(MASS)
library(dbscan)

set.seed(42)
```

### Załadowanie i przetwarzanie danych

```{r}
df_base = read.csv('housePrice.csv')
df = tibble(df_base)
df = df %>%
  mutate(Price = as.numeric(Price.USD.)) %>%
  dplyr::select(-c(Address, Price.USD.)) %>%
  mutate(Area = as.numeric(Area))
df = na.omit(df)
# konwertowanie zmiennych
df$Parking = as.integer(as.logical(df$Parking))
df$Warehouse = as.integer(as.logical(df$Warehouse))
df$Elevator = as.integer(as.logical(df$Elevator))

head(df)
```

Błąd wynika z bardzo dziwnych wartości dla zmiennej Area - te wartości są najprawdopodobniej błędem w zbiorze danych, więc to, że zostają zamienione na NULLe jest jak najbardziej w porządku

```{r}
ggplot(data=df, aes(x=Price, y=Area)) + geom_point()
```

### Wyznaczenie modeli na podstawie surowego zbioru danych

```{r}
MAE = function(actual, pred) {
  abs(sum(pred-actual))/length(actual)
}

calculate_models = function(df = df) {
# klasyczna regresja liniowa
model = lm(df$Price ~ df$Area + df$Room + df$Parking + df$Warehouse + df$Elevator, maxit=50)

# regresja na podstawie M estymatora na podstawie IWLS (iterated re-weight least squares)
model2 = rlm(df$Price ~ df$Area + df$Room + df$Parking + df$Warehouse + df$Elevator, scale.est='Huber', maxit=50)

# regresja na podstawie estymatora MM
model3 = rlm(df$Price ~ df$Area + df$Room + df$Parking + df$Warehouse + df$Elevator, method='MM', maxit=50)

print(
  data.frame(model = c('Klasyczna regresja', 'Regresja za pomocą M estymatora (IWLS)','Regresja za pomocą MM estymatora'), 
             AIC = c(AIC(model), AIC(model2), AIC(model3)),
             BIC = c(BIC(model), BIC(model2), BIC(model3)),
             RMSE = c(
                      mltools::rmse(df$Price, predict(model)),
                      mltools::rmse(df$Price, predict(model2)),
                      mltools::rmse(df$Price, predict(model3))
                      ),
             MAE = c(
                     MAE(df$Price, predict(model)),
                     MAE(df$Price, predict(model2)),
                     MAE(df$Price, predict(model3))
                     )
  )
)
}

calculate_models(df)
```

Wykorzystanie metod regresji odpornej tj. estymatora M i MM nie przyniosło poprawy w przypadku każdego z trzech mierników. Z przewidywaniem ceny najlepiej poradził sobie model klasycznej regresji liniowej, gdzie parametry szacowane są metodą MNK.

## Zastosowanie metod identyfikacji i usuwania outlierów

### Odległość Mahalanobisa

```{r}
df_mahalanobis <- df
df_mahalanobis$mahalanobis = mahalanobis(df, colMeans(df), cov(df))
df_mahalanobis$pvalue = pchisq(df_mahalanobis$mahalanobis, df=3, lower.tail=FALSE)
df_mahalanobis$removed_mah = ifelse(df_mahalanobis$pvalue < 0.001, 'removed', '')

ggplot(data=df_mahalanobis, aes(x=Price, y=Area, color = removed_mah)) + geom_point()
```

```{r}
print('Usuniete obserwacje:')
print(length(df$Price) - length(df_mahalanobis$Price))
```

Jako wartość graniczną p-value przyjęto 0.001 - usunięto w ten sposób 180 obserwacji

#### Wyznaczenie rozważanych modeli po usunięciu obserwacji odstających

```{r}
df_mahalanobis2 <- df_mahalanobis %>%
  filter(pvalue >= 0.001)

calculate_models(df=df_mahalanobis2)
```

Usunięcie tych obserwacji w znaczący sposób poprawia wyniki pod każdym względem, jednak dalej klasyczny model regresji najlepiej przewiduje cenę domu/mieszaknia.

### Local Outlier Factor (LOF)

```{r}
lof_scores = lof(df[,c(1,6)], minPts=10)

df_lof <- df
df_lof$lof_scores = lof_scores
df_lof$lof_outlier1 = ifelse(df_lof$lof_scores > 1, 'LOF outlier', '')
df_lof$lof_outlier5 = ifelse(df_lof$lof_scores > 5, 'LOF outlier', '')
df_lof$lof_outlier10 = ifelse(df_lof$lof_scores > 10, 'LOF outlier', '')

par(mfrow=c(1,3))
# usuniete obserwacje
ggplot(data=df_lof, aes(x=Price, y=Area, color = lof_outlier1)) + geom_point() + ggtitle('LOF Score > 1, usunięte obserwacje:', subtitle= length(filter(df_lof, lof_outlier1 == 'LOF outlier')$Price))
ggplot(data=df_lof, aes(x=Price, y=Area, color = lof_outlier5)) + geom_point() + ggtitle('LOF Score > 5,  usunięte obserwacje:', subtitle=length(filter(df_lof, lof_outlier5 == 'LOF outlier')$Price))
ggplot(data=df_lof, aes(x=Price, y=Area, color = lof_outlier10)) + geom_point() + ggtitle('LOF Score > 10,  usunięte obserwacje:', subtitle= length(filter(df_lof, lof_outlier10 == 'LOF outlier')$Price))

```

Wartość sugerowana przez autora pakietu w dokumentacji do przyjęcia jako kryterium odrzucenia obserwacji to LOF Score powyżej 1. Decydując się na to odrzucamy ponad połowę zbioru danych, więc przygotowane zostały także wersje z innymi wartościami krtyerium odrzucenia.

Widać wyraźną różnice w stosunku do stosowania odległości mahalanobisa, przy kryterium 5 i 10, odrzucane są głównie obserwacje, które zdecydowanie wyróżniają się przy danej cenie. Natomiast te wartości, gdzie jednocześnie rośnie cena i powierzchnia nie zostają wykluczone.

#### Oszacowane modele

-   dla LOF Score \> 1

```{r}
df_lof1 = df_lof %>%
  filter(lof_outlier1 == '')
calculate_models(df_lof1)
```

-   dla LOF Score \> 5

```{r}
df_lof5 = df_lof %>%
  filter(lof_outlier5 == '')
calculate_models(df_lof5)
```

-   dla LOF Score \> 10

```{r echo=TRUE}
df_lof10 = df_lof %>%
  filter(lof_outlier10 == '')
calculate_models(df_lof10)
```

Zastosowanie LOF poskutkowało zmniejszeniem się wszystkich mierników, jednak usunięto zdecydowanie więcej obserwacji niż w przypadku odległości Mahalanobisa (wtedy jedynie 180), a RMSE w każdym przypadku wynosi jedynie 1-5k mniej, gdy zastosowanie identyfikacji wartości odstających za pomocą odl. Mahalanobisa zmniejszyło RMSE do 10k przy usunięciu jedynie 180 obserwacji.

Oczywiście biorąc pod uwagę kontekst usuwania obserwacji, to nie musi zawsze oznaczać lepszej techniki, ponieważ usunięcie trudnych obserwacji, które pogarszają zdolności predykcyjne modelu =/= usunięcie rzeczywistych wartości odstających, których nie da się w żaden sposób wyjaśnić za pomocą posiadanych danych.
